{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sys\n",
    "import json\n",
    "# sys.path.append('../models/topic_model/')\n",
    "# import sampler_cy\n",
    "import os\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ace_phase2',\n",
       " 'Annotations.zip',\n",
       " 'combined_data_df.csv',\n",
       " 'finlayson_anno_offsets_new.json',\n",
       " 'finlayson_data.csv',\n",
       " 'glove.6B.zip',\n",
       " 'LDC2003T11.tgz',\n",
       " 'my_checked_annotations.csv',\n",
       " 'nyt_sample.csv',\n",
       " 'ruihong_news_structure_data',\n",
       " 'text_df.csv',\n",
       " 'XML']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.765489404641776"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/my_checked_annotations.csv', index_col=0).pipe(lambda df: \n",
    "    cohen_kappa_score(df['my_updated_labels'], df['finlayson_labels'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VERB    674\n",
       "CIRC    349\n",
       "MAIN    277\n",
       "EVAL    162\n",
       "CONS    157\n",
       "PREV    131\n",
       "LEAD     91\n",
       "HIST     90\n",
       "EXPE     61\n",
       "EXPL      5\n",
       "SECD      4\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/combined_data_df.csv')['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bnews', 'npaper', 'nwire']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/ace_phase2/data/ace2_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = json.load(open('../app/data/untagged_records.json'))\n",
    "corpus = pd.DataFrame(corpus).assign(doc_key=lambda df: df['doc_key'].str.replace('.sgm', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data['doc_key'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import spacy\n",
    "import langdetect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "def lang(x):\n",
    "    try:\n",
    "        return langdetect.detect(x)\n",
    "    except LangDetectException:\n",
    "        return 'na'\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, t2 = corpus.iloc[-5]\n",
    "sent_s = pd.Series([s.text for s in nlp(t2.replace('\\n', ' ')).sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By MATTHEW L. WALD &amp;LR;  &amp;QC;  &amp;QL; &amp;UR; c.199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The auditors, from the General Accounting Offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>At the aviation agency, the investigators foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The findings were the subject of a pair of GAO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In response, a State Department spokesman, Jam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The aviation agency, in contrast, said in a st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The agency's computers are aging, and the stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The auditors' report, however, said that ``whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>And they are certainly vulnerable to attack ``...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>While the usual mode of hacking is over the In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The agency ``has not assessed the physical sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>As for the report on the State Department, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>' The Senate committee's ranking Democrat, Joh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "4   By MATTHEW L. WALD &LR;  &QC;  &QL; &UR; c.199...\n",
       "5   The auditors, from the General Accounting Offi...\n",
       "6   At the aviation agency, the investigators foun...\n",
       "7   The findings were the subject of a pair of GAO...\n",
       "8   In response, a State Department spokesman, Jam...\n",
       "9   The aviation agency, in contrast, said in a st...\n",
       "11  The agency's computers are aging, and the stat...\n",
       "12  The auditors' report, however, said that ``whi...\n",
       "13  And they are certainly vulnerable to attack ``...\n",
       "14  While the usual mode of hacking is over the In...\n",
       "15  The agency ``has not assessed the physical sec...\n",
       "16  As for the report on the State Department, it ...\n",
       "17  ' The Senate committee's ranking Democrat, Joh..."
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sent_s\n",
    " .str.strip()\n",
    " .to_frame('text')\n",
    " .loc[lambda df: df['text'].apply(lang) == 'en']\n",
    " .loc[lambda df: ~df['text'].str.contains(key)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gigaword_corpus = []\n",
    "for article_file in glob.glob('../data/ruihong_news_structure_data/sampled/*/*'):\n",
    "    t2 = open(article_file).read().strip()\n",
    "    soup = BeautifulSoup(t2)\n",
    "    article_type = re.findall('type=\\\"(.*?)\\\"', t2)[0]\n",
    "#     article_class = re.findall('class=\\\"(.*?)\\\"', t2)[0]\n",
    "    article_id = re.findall('id=\\\"(.*?)\\\"', t2)[0]\n",
    "    paragraphs = list(map(lambda x: x.text.strip().replace('\\n', ' '), soup.find_all('p')))\n",
    "    \n",
    "    for p in paragraphs:\n",
    "        gigaword_corpus.append({'article_id': article_id, 'text': p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gigaword_corpus_paragraphs = pd.DataFrame(gigaword_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2004, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/combined_data_df.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17811, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gigaword_corpus_paragraphs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (1,4,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "nyt_corpus = pd.read_csv('../../newspaper-pages/nyt_corpus/unprocessed_full_data_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_sample = (nyt_corpus\n",
    " .loc[lambda df: df['print_section_and_page'] == 'A-001']\n",
    " .loc[lambda df: df['body'].notnull()]\n",
    " [['id', 'body']].sample(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_sample.to_csv('../data/nyt_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_sample = pd.read_csv('../data/nyt_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import unique_everseen\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_sentences = []\n",
    "for _, _, doc_id, text in tqdm(nyt_sample.itertuples(), total=len(nyt_sample)):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    doc = nlp(text)\n",
    "    sents = [x.text for x in doc.sents]\n",
    "    sents = list(unique_everseen(sents))\n",
    "    for s in sents:\n",
    "        nyt_sentences.append({'id': doc_id, 'text': s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_sentences_corpus = pd.DataFrame(nyt_sentences).loc[lambda df: df['text'].str.strip().str.len() > 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data For Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_sentences_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NYT_ENG_20100108.0149</td>\n",
       "      <td>It's official: China is the world's largest ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYT_ENG_20100108.0149</td>\n",
       "      <td>Passenger car sales totaled 10.3 million in Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NYT_ENG_20100108.0149</td>\n",
       "      <td>By contrast, U.S. car and light truck sales sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NYT_ENG_20100108.0149</td>\n",
       "      <td>Demand for vehicles picked up in the U.S. in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYT_ENG_20100108.0149</td>\n",
       "      <td>Chinese auto sales began to take off early in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              article_id                                               text\n",
       "0  NYT_ENG_20100108.0149  It's official: China is the world's largest ve...\n",
       "1  NYT_ENG_20100108.0149  Passenger car sales totaled 10.3 million in Ch...\n",
       "2  NYT_ENG_20100108.0149  By contrast, U.S. car and light truck sales sh...\n",
       "3  NYT_ENG_20100108.0149  Demand for vehicles picked up in the U.S. in t...\n",
       "4  NYT_ENG_20100108.0149  Chinese auto sales began to take off early in ..."
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gigaword_corpus_paragraphs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_ace_df = pd.read_csv('../data/combined_data_df.csv')[['doc_key', 'paragraph_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.concat([\n",
    "    tagged_ace_df.rename(columns={'paragraph_text':'text'}),\n",
    "    gigaword_corpus_paragraphs.rename(columns={'article_id': 'doc_key'}),\n",
    "    nyt_sentences_corpus.rename(columns={'id': 'doc_key'})\n",
    "], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv('../data/text_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72631, 5)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df=25, max_df=.5, stop_words='english', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.5, max_features=None, min_df=25,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(text_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paragraph(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, ' ')\n",
    "    output = []\n",
    "    for t in text.split():\n",
    "        if t in vocab:\n",
    "            output.append(int(vocab.get(t)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['vectors'] = text_df['text'].str.lower().apply(process_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {\n",
    " 'LEAD': 0,\n",
    " 'CIRC': 1,\n",
    " 'MAIN': 2,\n",
    " 'VERB': 3,\n",
    " 'CONS': 4,\n",
    " 'PREV': 5,\n",
    " 'EVAL': 6,\n",
    " 'HIST': 7,\n",
    " 'EXPE': 8,\n",
    " 'SECD': 9,\n",
    " 'EXPL': 10,\n",
    " np.nan: -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['label_id'] = text_df['label'].map(label_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = text_df.groupby('doc_key')[['vectors', 'label_id']].aggregate(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df['has_labels'] = doc_df['label_id'].apply(lambda x: not all(map(lambda y: y==-1, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/topic_model/labeled_data/index.txt', 'w') as f:\n",
    "    for idx in list(doc_df.index):\n",
    "        f.write(idx)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = doc_df.rename(columns={'vectors':'paragraphs', 'label_id':'labels'}).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/topic_model/labeled_data/doc_vecs.json', 'w') as f:\n",
    "    for j in jsons:\n",
    "        f.write(json.dumps(j, default=convert))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/topic_model/labeled_data/vocab.txt', 'w') as f:\n",
    "    for word in pd.Series(cv.vocabulary_).sort_values().index:\n",
    "        f.write(word)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sampler_cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/topic_model/labeled_data/doc_vecs.json') as f:\n",
    "    for line in f:\n",
    "        t = json.loads(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data for BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('../data/combined_data_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = (data_df\n",
    " .loc[lambda df: df['label'].notnull()]\n",
    " .loc[lambda df: df['label'].isin(df['label'].value_counts()[:-2].index)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_record(record):\n",
    "    output = []\n",
    "    doc_key = record['doc_key']\n",
    "    label = record['label']\n",
    "    sents = list(nlp(' '.join(record['paragraph_text'].split())).sents)\n",
    "    for s in sents:\n",
    "        output.append({\n",
    "            'doc_key': doc_key,\n",
    "            'label': label,\n",
    "            'paragraph_text': s.text\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f056c5645d94444c966f955e4973f28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1992), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_records = []\n",
    "for row in tqdm(data_df.to_dict(orient='records')):\n",
    "    all_records += process_record(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = (\n",
    "    pd.DataFrame(all_records)\n",
    "        .loc[lambda df: df['paragraph_text'].str.len() > 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (final_df.groupby('doc_key')[['paragraph_text', 'label']].aggregate(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_test = train_test_split(grouped.index, test_size=0.1)\n",
    "idx_train, idx_val = train_test_split(idx_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/bilstm/data/train_keys.txt', 'w') as train_keys, \\\n",
    "open('../models/bilstm/data/train_keys.txt', 'w') as test_keys, \\\n",
    "open('../models/bilstm/data/train_keys.txt', 'w') as val_keys, \\\n",
    "open('../models/bilstm/data/train_input.txt', 'w') as train_f, \\\n",
    "open('../models/bilstm/data/test_input.txt', 'w') as test_f, \\\n",
    "open('../models/bilstm/data/val_input.txt', 'w') as val_f:\n",
    "    for doc_key, paragraphs, labels in grouped.itertuples():\n",
    "        if doc_key in idx_train:\n",
    "            f = train_f\n",
    "            keys = train_keys\n",
    "        elif doc_key in idx_test:\n",
    "            f = test_f\n",
    "            keys = test_keys\n",
    "        elif doc_key in idx_val:\n",
    "            f = val_f\n",
    "            keys = val_keys\n",
    "        else:\n",
    "            print('not found...')\n",
    "\n",
    "        for par, label in zip(paragraphs, labels):\n",
    "            f.write('%s\\t%s\\n' % (' '.join(par.split()), label))\n",
    "        f.write('\\n')\n",
    "        keys.write(doc_key)\n",
    "        keys.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import get_pretrained, PretrainedList, get_checkpoint_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PretrainedList.wwm_uncased_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\.keras\\datasets\\wwm_uncased_L-24_H-1024_A-16\\bert_config.json C:\\Users\\alexa\\.keras\\datasets\\wwm_uncased_L-24_H-1024_A-16\\bert_model.ckpt C:\\Users\\alexa\\.keras\\datasets\\wwm_uncased_L-24_H-1024_A-16\\vocab.txt\n"
     ]
    }
   ],
   "source": [
    "model_path = get_pretrained(PretrainedList.wwm_uncased_large)\n",
    "paths = get_checkpoint_paths(model_path)\n",
    "print(paths.config, paths.checkpoint, paths.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
